<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Hadoop环境配置 | Dem0のblog</title><meta name="keywords" content="大数据处理"><meta name="author" content="Dem0"><meta name="copyright" content="Dem0"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Hadoop1 安装配置过程1.1 安装配置树莓派(1). 软硬件准备  树莓派   SD 卡格式工具   树莓派官方系统烧录工具   树莓派操作系统，建议选择官方系统   (2). 烧录软件  配置ssh和设置wifi(对应的主机名分别设置为master,slave01,slave02)   (3). 连接配置网络 ​	打开手机下载软件某热点软件，可以查看树莓派的ip ​	 ​	然后输入ssh">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop环境配置">
<meta property="og:url" content="https://dem0dem0.top/2022/06/13/hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/index.html">
<meta property="og:site_name" content="Dem0のblog">
<meta property="og:description" content="Hadoop1 安装配置过程1.1 安装配置树莓派(1). 软硬件准备  树莓派   SD 卡格式工具   树莓派官方系统烧录工具   树莓派操作系统，建议选择官方系统   (2). 烧录软件  配置ssh和设置wifi(对应的主机名分别设置为master,slave01,slave02)   (3). 连接配置网络 ​	打开手机下载软件某热点软件，可以查看树莓派的ip ​	 ​	然后输入ssh">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img.dem0dem0.top/images/image-20220618082641264.png">
<meta property="article:published_time" content="2022-06-13T12:10:23.000Z">
<meta property="article:modified_time" content="2022-06-18T01:35:13.956Z">
<meta property="article:author" content="Dem0">
<meta property="article:tag" content="大数据处理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img.dem0dem0.top/images/image-20220618082641264.png"><link rel="shortcut icon" href="https://q1.qlogo.cn/g?b=qq&nk=79475432&s=640"><link rel="canonical" href="https://dem0dem0.top/2022/06/13/hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop环境配置',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-06-18 09:35:13'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="Dem0のblog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://q1.qlogo.cn/g?b=qq&amp;nk=79475432&amp;s=640" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movie/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> Book</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img.dem0dem0.top/images/image-20220618082641264.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Dem0のblog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movie/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> Book</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Hadoop环境配置</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-06-13T12:10:23.000Z" title="发表于 2022-06-13 20:10:23">2022-06-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-06-18T01:35:13.956Z" title="更新于 2022-06-18 09:35:13">2022-06-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/">大数据处理</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Hadoop环境配置"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h2 id="1-安装配置过程"><a href="#1-安装配置过程" class="headerlink" title="1 安装配置过程"></a>1 安装配置过程</h2><h3 id="1-1-安装配置树莓派"><a href="#1-1-安装配置树莓派" class="headerlink" title="1.1 安装配置树莓派"></a>1.1 安装配置树莓派</h3><p>(1). 软硬件准备</p>
<ol>
<li><p>树莓派 </p>
</li>
<li><p>SD 卡格式工具 </p>
</li>
<li><p>树莓派官方系统烧录工具 </p>
</li>
<li><p>树莓派操作系统，建议选择官方系统</p>
</li>
</ol>
<p>(2). 烧录软件</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613092648337.png" alt="image-20220613092648337"></p>
<p>配置ssh和设置wifi(对应的主机名分别设置为<code>master,slave01,slave02</code>)</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613092723152.png" alt="image-20220613092723152"></p>
<p><img src="https://img.dem0dem0.top/images/image-20220613092747268.png" alt="image-20220613092747268"></p>
<p>(3). 连接配置网络</p>
<p>​	打开手机下载软件<code>某热点软件</code>，可以查看树莓派的ip</p>
<p>​	<img src="https://img.dem0dem0.top/images/image-20220613111801789.png" alt="image-20220613111801789"></p>
<p>​	然后输入<code>ssh nudt@192.168.225.211</code>.输入密码<code>ssh</code></p>
<h3 id="1-2-安装配置jdk"><a href="#1-2-安装配置jdk" class="headerlink" title="1.2 安装配置jdk"></a>1.2 安装配置jdk</h3><p>(1). 将下载的<code>jdk-8u241-linux-arm64-vfp-hflt.tar.gz</code>,通过<code>termius</code>传递到三台树莓派上。</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613113424523.png" alt="image-20220613113424523"></p>
<p>(2). 解压</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf jdk-8u241-linux-arm64-vfp-hflt.tar.gz</span><br><span class="line">sudo <span class="built_in">mkdir</span> /usr/lib/jvm/</span><br><span class="line">sudo <span class="built_in">mv</span> jdk1.8.0_241/ /usr/lib/jvm/</span><br></pre></td></tr></table></figure>

<p>(3). 配置环境变量</p>
<p>配置的文件为<code>/etc/profile</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sudo vim /etc/profile</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.8.0_241</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="string">&quot;.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$CLASSPATH</span>&quot;</span> </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span>&quot;</span></span><br></pre></td></tr></table></figure>

<p>使他生效</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p>(4). 设置系统默认jdk</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.8.0_241/bin/java 300 </span><br><span class="line">sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jdk1.8.0_241/bin/javac 300 </span><br><span class="line">sudo update-alternatives --install /usr/bin/jar jar /usr/lib/jvm/jdk1.8.0_241/bin/jar 300  </span><br><span class="line">sudo update-alternatives --install /usr/bin/javah javah /usr/lib/jvm/jdk1.8.0_241/bin/javah 300  </span><br><span class="line">sudo update-alternatives --install /usr/bin/javap javap /usr/lib/jvm/jdk1.8.0_241/bin/javap 300</span><br><span class="line">sudo update-alternatives --config java</span><br></pre></td></tr></table></figure>

<p>(5). 验证java安装成功</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613115053096.png" alt="image-20220613115053096"></p>
<h3 id="1-3-安装配置hadoop"><a href="#1-3-安装配置hadoop" class="headerlink" title="1.3 安装配置hadoop"></a>1.3 安装配置hadoop</h3><p>(1). 下载（<strong>只需要在master上做</strong>）</p>
<p>或者使用压缩包</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget --no-check-certificate https://repo.huaweicloud.com/apache/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz</span><br></pre></td></tr></table></figure>

<p>可以使用我提供的压缩包。</p>
<p>(2). 解压(<strong>只需要在master上做</strong>)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf hadoop-3.3.2.tar.gz</span><br><span class="line"><span class="built_in">mv</span> hadoop-3.3.2 ~/hadoop/</span><br></pre></td></tr></table></figure>

<p>(3). 配置并启动hadoop的环境变量，<code>source /etc/profile</code>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/nudt/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> YARN_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="variable">$HADOOP_HOME</span>/lib/native</span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">&quot;-Djava.library.path-<span class="variable">$HADOOP_HOME</span>/lib&quot;</span></span><br><span class="line"><span class="built_in">export</span> JAVA_LIBRARY_PATH=<span class="variable">$HADOOP_HOME</span>/lib/native:<span class="variable">$JAVA_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure>

<p>(4). 验证hadoop是否安装成功(<strong>只需要在master</strong>)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hadoop</span><br><span class="line">Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]</span><br><span class="line"> or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]</span><br><span class="line">  where CLASSNAME is a user-provided Java class</span><br><span class="line"></span><br><span class="line">  OPTIONS is none or any of:</span><br><span class="line"></span><br><span class="line">buildpaths                       attempt to add class files from build tree</span><br><span class="line">--config dir                     Hadoop config directory</span><br><span class="line">--debug                          turn on shell script debug mode</span><br><span class="line">--help                           usage information</span><br><span class="line">hostnames list[,of,host,names]   hosts to use in slave mode</span><br><span class="line">hosts filename                   list of hosts to use in slave mode</span><br><span class="line">loglevel level                   set the log4j level for this command</span><br><span class="line">workers                          turn on worker mode</span><br><span class="line"></span><br><span class="line">  SUBCOMMAND is one of:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    Admin Commands:</span><br><span class="line"></span><br><span class="line">daemonlog     get/set the log level for each daemon</span><br><span class="line"></span><br><span class="line">    Client Commands:</span><br><span class="line"></span><br><span class="line">archive       create a Hadoop archive</span><br><span class="line">checknative   check native Hadoop and compression libraries availability</span><br><span class="line">classpath     prints the class path needed to get the Hadoop jar and the</span><br><span class="line">              required libraries</span><br><span class="line">conftest      validate configuration XML files</span><br><span class="line">credential    interact with credential providers</span><br><span class="line">distch        distributed metadata changer</span><br><span class="line">distcp        copy file or directories recursively</span><br><span class="line">dtutil        operations related to delegation tokens</span><br><span class="line">envvars       display computed Hadoop environment variables</span><br><span class="line">fs            run a generic filesystem user client</span><br><span class="line">gridmix       submit a mix of synthetic job, modeling a profiled from</span><br><span class="line">              production load</span><br><span class="line">jar &lt;jar&gt;     run a jar file. NOTE: please use &quot;yarn jar&quot; to launch YARN</span><br><span class="line">              applications, not this command.</span><br><span class="line">jnipath       prints the java.library.path</span><br><span class="line">kdiag         Diagnose Kerberos Problems</span><br><span class="line">kerbname      show auth_to_local principal conversion</span><br><span class="line">key           manage keys via the KeyProvider</span><br><span class="line">rumenfolder   scale a rumen input trace</span><br><span class="line">rumentrace    convert logs into a rumen trace</span><br><span class="line">s3guard       manage metadata on S3</span><br><span class="line">trace         view and modify Hadoop tracing settings</span><br><span class="line">version       print the version</span><br><span class="line"></span><br><span class="line">    Daemon Commands:</span><br><span class="line"></span><br><span class="line">kms           run KMS, the Key Management Server</span><br><span class="line">registrydns   run the registry DNS server</span><br><span class="line"></span><br><span class="line">SUBCOMMAND may print help when invoked w/o parameters or with -h.</span><br></pre></td></tr></table></figure>

<p>(5). :star:修改主机名和配置网络映射</p>
<p>主机名在初始化配置的时候我已经要求设置了，只能做重复讲解。<code>sudo vim /etc/hostname</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">master或者slave01或者slave02</span><br></pre></td></tr></table></figure>

<p>修改<code>sudo vim /etc/hosts</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">192.168.239.28 master</span><br><span class="line">192.168.239.211 slave01</span><br><span class="line">192.168.239.254 slave02</span><br></pre></td></tr></table></figure>

<p>修改网络映射<code>sudo vim /etc/cloud/templates/hosts.debian.tmpl</code>（<strong>注意!!!只保留下面这些数据，其他ipv4的数据全删除！！！）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## template:jinja</span><br><span class="line">&#123;#</span><br><span class="line">This file (/etc/cloud/templates/hosts.debian.tmpl) is only utilized</span><br><span class="line">if enabled in cloud-config.  Specifically, in order to enable it</span><br><span class="line">you need to add the following to config:</span><br><span class="line">   manage_etc_hosts: True</span><br><span class="line">-#&#125;</span><br><span class="line"># Your system has configured &#x27;manage_etc_hosts&#x27; as True.</span><br><span class="line"># As a result, if you wish for changes to this file to persist</span><br><span class="line"># then you will need to either</span><br><span class="line"># a.) make changes to the master file in /etc/cloud/templates/hosts.debian.tmpl</span><br><span class="line"># b.) change or remove the value of &#x27;manage_etc_hosts&#x27; in</span><br><span class="line">#     /etc/cloud/cloud.cfg or cloud-config from user-data</span><br><span class="line">#</span><br><span class="line">&#123;# The value &#x27;&#123;&#123;hostname&#125;&#125;&#x27; will be replaced with the local-hostname -#&#125;</span><br><span class="line">#127.0.1.1 &#123;&#123;fqdn&#125;&#125; &#123;&#123;hostname&#125;&#125;</span><br><span class="line">#127.0.0.1 localhost</span><br><span class="line">192.168.239.28 master</span><br><span class="line">192.168.239.211 slave01</span><br><span class="line">192.168.239.254 slave02</span><br><span class="line"># The following lines are desirable for IPv6 capable hosts</span><br><span class="line">::1 localhost ip6-localhost ip6-loopback</span><br><span class="line">ff02::1 ip6-allnodes</span><br><span class="line">ff02::2 ip6-allrouters</span><br></pre></td></tr></table></figure>

<p><strong>注意这里的网段设置一定要和你ssh连接上去的同步</strong>最后</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp /etc/cloud/templates/hosts.debian.tmpl root@slave01:/etc/cloud/templates/hosts.debian.tmpl</span><br><span class="line">scp /etc/cloud/templates/hosts.debian.tmpl root@slave02:/etc/cloud/templates/hosts.debian.tmpl</span><br><span class="line">scp /etc/hosts root@slave01:/etc/hosts</span><br><span class="line">scp /etc/hosts root@slave02:/etc/hosts</span><br></pre></td></tr></table></figure>

<p>(6). 配置ssh免密登录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa(一直回车就行)</span><br><span class="line">cd /home/nudt/.ssh</span><br><span class="line">cat id_rsa.pub  &gt;&gt; authorized_keys</span><br><span class="line">ssh-copy-id -i ./id_rsa.pub  nudt@slave01(这里就是一台主机对于两外两台)</span><br><span class="line">ssh-copy-id -i ./id_rsa.pub  nudt@slave02</span><br></pre></td></tr></table></figure>

<p>现在可以开到下面这些文件</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613193638490.png" alt="image-20220613193638490"></p>
<p>这里我们需要将所有的主机之间开通</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613194001823.png" alt="image-20220613194001823"></p>
<p>下面测试远程登陆免密码。</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613194032730.png" alt="image-20220613194032730"></p>
<p>测试成功。</p>
<p>(7).配置hadoop</p>
<p>​	a). 配置core-site.xml</p>
<p>​		该配置文件属于 Hadoop 的全局配置文件，我们主要进行配 置分布式文件系统 HDFS 的入口地址（即 NameNode 的地址）和 HDFS 运行时所生产数 据的保存位置</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/nudt/hadoop/etc/hadoop</span><br><span class="line">vim core-site.xml</span><br><span class="line"><span class="built_in">mkdir</span> /home/nudt/hadoop_data/</span><br><span class="line"><span class="built_in">mkdir</span> /home/nudt/hadoop_data/tmp</span><br><span class="line"><span class="built_in">mkdir</span> /home/nudt/hadoop_data/dfs/</span><br><span class="line"><span class="built_in">mkdir</span> /home/nudt/hadoop_data/dfs/name</span><br><span class="line"><span class="built_in">mkdir</span> /home/nudt/hadoop_data/dfs/data</span><br><span class="line">sudo <span class="built_in">mkdir</span> /usr/container/logs</span><br></pre></td></tr></table></figure>

<p>将下面的内容修改后粘贴进去</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定 HDFS 中 NameNode 的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 其中 hdfs 为协议名称，master 为 namenode 的节点主机名称，端口号为9000 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定 hadoop 运行时产生文件的存储目录，该目录需要单独创建 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/nudt/hadoop_data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>参数说明</p>
<ul>
<li>fs.defaultFS(指定 HDFS 中 NameNode 的地址)</li>
<li>hadoop.tmp.dir(指定 hadoop 运行时产生文件的存储目录)</li>
</ul>
<p>​	b). 配置 hdfs-site.xml 文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该参数是配置 NameNode 的 http 访问地址和端口号 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.239.28:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该参数是配置 该参数是配置 SecondaryNameNode 的 http 访</span></span><br><span class="line"><span class="comment">问地址和端口号 的 http 访问地址和端口号 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.239.28:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该参数是配置 HDFS 副本数量。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该参数是设置 NameNode 存放的路径。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/nudt/hadoop_data/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- dfs.datanode.data.dir：该参数是设置 DataNode 存放的路径。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/nudt/hadoop_data/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这里只有一个参数需要说明，因为我们一共3台机器，配置只有一个master和两个slave，所有<code>secondaryNameNode</code>也是master.</p>
<p>​	c) yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostsname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  ResourceManager 服务器的</span></span><br><span class="line"><span class="comment">web 地址和端口。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 NodeManager 启动时加载 server 的 方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定使用 mapreduce_shuffle 中的类。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置是否启用日志聚集功能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置聚集的日志在 HDFS 上保存的最长时间。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>106800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定日志聚合目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/container/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​	d) mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.intermediate-done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;hadoop.tmp.dir&#125;/mr-history/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;hadoop.tmp.dir&#125;/mr-history/done<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​	e) 配置workes</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">master</span><br><span class="line">slave01</span><br><span class="line">slave02</span><br></pre></td></tr></table></figure>

<p>​	f)现在开始分发到各个从机器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp -r  /home/nudt/hadoop nudt@slave01:/home/nudt/</span><br><span class="line">scp -r /home/nudt/hadoop nudt@slave02:/home/nudt/</span><br></pre></td></tr></table></figure>

<p>​	时间比较长，耐心等待….</p>
<h3 id="1-4-验证安装"><a href="#1-4-验证安装" class="headerlink" title="1.4 验证安装"></a>1.4 验证安装</h3><p>在启动hadoop集群之前需要先格式化namenode</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p>1）启动和停止 HDFS</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-dfs.sh <span class="comment"># 启动 HDFS</span></span><br><span class="line">stop-dfs.sh <span class="comment"># 停止 HDFS</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>启动和停止 Yarn</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-yarn.sh <span class="comment"># 启动 YARN</span></span><br><span class="line">stop-yarn.sh <span class="comment"># 停止 YARN</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>全部暂停或启动</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-all.sh <span class="comment"># 启动 HDFS 和 YARN</span></span><br><span class="line">stop-all.sh <span class="comment"># 停止 HDFS 和 YARN</span></span><br></pre></td></tr></table></figure>

<ol start="4">
<li>启动和停止历史（日志）服务器</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mr-jobhistory-daemon.sh start historyserver <span class="comment"># 启动 historyserver</span></span><br><span class="line">mr-jobhistory-daemon.sh start historyserver <span class="comment"># 停止 historyserver</span></span><br></pre></td></tr></table></figure>

<ol start="5">
<li>查看jbs</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<h3 id="1-5-交换机配置-可选"><a href="#1-5-交换机配置-可选" class="headerlink" title="1.5 交换机配置(可选)"></a>1.5 交换机配置(可选)</h3><p>(1). 配置windows机器的固定ip</p>
<p>右键打开网络设置，选择<code>更改适配器</code></p>
<p><img src="https://img.dem0dem0.top/images/image-20220613204419805.png" alt="image-20220613204419805"></p>
<p>选择以太网，右键<code>属性</code>，选择<code>interel桥接协议ipv4</code></p>
<p><img src="https://img.dem0dem0.top/images/image-20220613205450441.png" alt="image-20220613205450441"></p>
<p>按照如下设置</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613205533184.png" alt="image-20220613205533184"></p>
<p>(2).配置树莓派的三台ip（注意和之前的保持一致)</p>
<ol>
<li><p>插入电脑，打开system-boot盘<br><img src="https://img.dem0dem0.top/images/image-20220613205627741.png" alt="image-20220613205627741"></p>
</li>
<li><p>编辑cmdline.txt</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613205714223.png" alt="image-20220613205714223"></p>
</li>
</ol>
<p>添加图中这一行，重复此步骤，<strong>注意</strong>一定要和上面的对应起来，不然你都得全配。然后一切恢复正常。</p>
<h3 id="1-6-问题解决"><a href="#1-6-问题解决" class="headerlink" title="1.6 问题解决"></a>1.6 问题解决</h3><h4 id="a-JAVA-HOME没有设置"><a href="#a-JAVA-HOME没有设置" class="headerlink" title="a. JAVA_HOME没有设置"></a>a. JAVA_HOME没有设置</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /home/nudt/hadoop/etc/hadoop/hadoop-env.sh</span><br><span class="line">(加入这一句)<span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.8.0_241</span><br></pre></td></tr></table></figure>

<h4 id="b-master-Permission-denied"><a href="#b-master-Permission-denied" class="headerlink" title="b. master: Permission denied"></a>b. master: Permission denied</h4><p>原因不是权限不够，而是<code>未将master所使用的用户的公钥加到相应主机下面</code></p>
<ul>
<li><p>先调整到root用户</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo passwd(密码设置为root)</span><br><span class="line">su root</span><br></pre></td></tr></table></figure>
</li>
<li><p>生成密钥对</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -P <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>写入本机的root的信任</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
</li>
<li><p>发送公钥</p>
<p>首先在slave机器上也执行<code>调整为root过程</code>，修改<code>/etc/ssh/sshd_config</code>文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure>

<p>找到<code>PermitRootLogin prohibit-password</code>.</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613234132124.png" alt="image-20220613234132124"></p>
<p>将这个修改为如下</p>
<p><img src="https://img.dem0dem0.top/images/image-20220613234332312.png" alt="image-20220613234332312"></p>
<p>重启服务<code>service sshd restart</code>.然后后面操作的时候都用<code>root</code>用户！。</p>
</li>
<li><p>最后添加配置(root用户)</p>
<p><code>sudo vim /etc/profile</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>

<p>最后加入这个。<strong>这里你想用什么用户登录创建，你就用什么用户，不必用root</strong></p>
</li>
</ul>
<h4 id="c-解决could-only-be-written-to-0-of-the-1-minReplicati"><a href="#c-解决could-only-be-written-to-0-of-the-1-minReplicati" class="headerlink" title="c. 解决could only be written to 0 of the 1 minReplicati"></a>c. 解决<code>could only be written to 0 of the 1 minReplicati</code></h4><blockquote>
<p>参考:<a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_38737592/article/details/101628357">https://blog.csdn.net/sinat_38737592/article/details/101628357</a></p>
</blockquote>
<h2 id="2-hadoop使用"><a href="#2-hadoop使用" class="headerlink" title="2. hadoop使用"></a>2. hadoop使用</h2><h3 id="2-1-hadoop文件基础操作"><a href="#2-1-hadoop文件基础操作" class="headerlink" title="2.1 hadoop文件基础操作"></a>2.1 hadoop文件基础操作</h3><h4 id="SHELL-命令"><a href="#SHELL-命令" class="headerlink" title="SHELL 命令"></a>SHELL 命令</h4><p>远程访问的时候增加参数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides &#x27;fs.defaultFS&#x27; property from configurations.</span><br><span class="line">-fs hdfs://master:9000 </span><br></pre></td></tr></table></figure>

<p>创建文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">touch</span> /tmp/exp.tx</span><br></pre></td></tr></table></figure>

<p>写入文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;&lt;Text to append&gt;&quot;</span> | hadoop fs -appendToFile - /aaa/aa.txt </span><br><span class="line">hadoop fs -appendToFile &#123;src&#125; &#123;dst&#125;</span><br></pre></td></tr></table></figure>

<p>删除文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">rm</span> README.txt</span><br></pre></td></tr></table></figure>

<p>下载文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[-get [-f] [-p] [-crc] [-ignoreCrc] [-t &lt;thread count&gt;] [-q &lt;thread pool queue size&gt;] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">hadoop fs -get &lt;src&gt; &lt;localdst&gt;</span><br><span class="line">最重要-t 可以设置进程</span><br></pre></td></tr></table></figure>

<p>重命名&#x2F;移动</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-<span class="built_in">mv</span></span><br></pre></td></tr></table></figure>

<p>复制</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-cp</span><br></pre></td></tr></table></figure>

<p>查看详细信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-cat</span><br></pre></td></tr></table></figure>

<p>配置权限(！！必须做！！)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">chmod</span> 777 /</span><br></pre></td></tr></table></figure>

<p>更多查看 <code>hadoop fs --help</code></p>
<h4 id="java代码操作"><a href="#java代码操作" class="headerlink" title="java代码操作"></a>java代码操作</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Progressable;</span><br><span class="line"><span class="keyword">import</span> org.apache.kerby.util.PublicKeyDeriver;</span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FileManager</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">String</span> <span class="variable">nameNode</span> <span class="operator">=</span> <span class="string">&quot;192.168.239.28:9000&quot;</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> URI hdfsHost;</span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            hdfsHost = <span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://192.168.239.28:9000&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (URISyntaxException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span>  <span class="keyword">void</span> <span class="title function_">createHelloWorld</span><span class="params">(Configuration cf,String filePath)</span> <span class="keyword">throws</span> IOException, URISyntaxException &#123;</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(hdfsHost,cf);</span><br><span class="line">        <span class="type">byte</span>[] buff = <span class="string">&quot;Hello World&quot;</span>.getBytes(StandardCharsets.UTF_8);</span><br><span class="line">        <span class="keyword">if</span>(!fs.exists(<span class="keyword">new</span> <span class="title class_">Path</span>(filePath)))&#123;</span><br><span class="line">            <span class="type">FSDataOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> fs.create(<span class="keyword">new</span> <span class="title class_">Path</span>(filePath));</span><br><span class="line">            fos.write(buff,<span class="number">0</span>,buff.length);</span><br><span class="line">            System.out.println(<span class="string">&quot;Create a new File:&quot;</span> + filePath +<span class="string">&quot; with HelloWord&quot;</span>);</span><br><span class="line">            fos.close();</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Will Overwrti file:\t&quot;</span> +  filePath);</span><br><span class="line">            System.out.println(<span class="string">&quot;Add  contents to :\t&quot;</span> + filePath +<span class="string">&quot; with HelloWord&quot;</span>);</span><br><span class="line">            <span class="type">FSDataOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> fs.create(<span class="keyword">new</span> <span class="title class_">Path</span>(filePath),<span class="literal">true</span>);</span><br><span class="line">            fos.write(buff,<span class="number">0</span>,buff.length);</span><br><span class="line">            fos.close();</span><br><span class="line">        &#125;</span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">fileExist</span><span class="params">(Configuration cf, String filePath)</span> <span class="keyword">throws</span> IOException&#123;</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(hdfsHost,cf);</span><br><span class="line">        <span class="keyword">if</span>(fs.exists(<span class="keyword">new</span> <span class="title class_">Path</span>(filePath)))&#123;</span><br><span class="line">            System.out.println(filePath + <span class="string">&quot;\tExists!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            System.out.println(filePath + <span class="string">&quot;\tNot Exists!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span>  <span class="keyword">void</span> <span class="title function_">readFile</span><span class="params">(Configuration cf,String filePath)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(hdfsHost,cf);</span><br><span class="line">        <span class="type">FSDataInputStream</span> <span class="variable">open</span> <span class="operator">=</span> fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(filePath));</span><br><span class="line">        <span class="type">BufferedReader</span> <span class="variable">bfr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(open));</span><br><span class="line">        System.out.println(<span class="string">&quot;Begin Read:&quot;</span> + filePath);</span><br><span class="line">        <span class="type">String</span> <span class="variable">contentLine</span> <span class="operator">=</span> bfr.readLine();</span><br><span class="line">        <span class="keyword">while</span> (contentLine != <span class="literal">null</span>) &#123;</span><br><span class="line">            System.out.println(contentLine);</span><br><span class="line">            contentLine = bfr.readLine();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 删除的路径Path f，一个是是否递归（recursive）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">delteFile</span><span class="params">(Configuration cf,String filePath)</span> <span class="keyword">throws</span> IOException&#123;</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span>FileSystem.get(hdfsHost,cf);</span><br><span class="line">        <span class="keyword">if</span>(fs.delete(<span class="keyword">new</span> <span class="title class_">Path</span>(filePath), <span class="literal">false</span>))&#123;</span><br><span class="line">            System.out.println(filePath + <span class="string">&quot;\tdelete success!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            System.out.println(filePath + <span class="string">&quot;\tdelete fail!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">showDir</span><span class="params">(Configuration cf , String filePath)</span> <span class="keyword">throws</span> IOException&#123;</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span>FileSystem.get(hdfsHost,cf); </span><br><span class="line">        RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> <span class="title class_">Path</span>(filePath), <span class="literal">false</span>);</span><br><span class="line">        <span class="comment">//递归列出该目录下所有文件，不包括文件夹，后面的布尔值为是否递归</span></span><br><span class="line">		<span class="keyword">while</span>(listFiles.hasNext()) &#123;<span class="comment">//如果listfiles里还有东西</span></span><br><span class="line">			<span class="type">LocatedFileStatus</span> <span class="variable">next</span> <span class="operator">=</span> listFiles.next();<span class="comment">//得到下一个并pop出listFiles</span></span><br><span class="line">			System.out.println(next.getPath().getName());<span class="comment">//输出</span></span><br><span class="line">		&#125;</span><br><span class="line">        <span class="comment">//列出目录下所有的文件以及文件夹</span></span><br><span class="line">        FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(filePath));<span class="comment">//获取目录状态</span></span><br><span class="line">		<span class="keyword">for</span>(FileStatus list:listStatus) &#123;<span class="comment">//增强for循环遍历listStatus</span></span><br><span class="line">			System.out.println(list.getPath().getName());<span class="comment">//输出</span></span><br><span class="line">		&#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">uploadFile</span><span class="params">(Configuration cf, String localstr, String dst)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span>FileSystem.get(hdfsHost,cf); </span><br><span class="line">        <span class="type">InputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(localstr);</span><br><span class="line">        <span class="type">OutputStream</span> <span class="variable">out</span> <span class="operator">=</span> fs.create(<span class="keyword">new</span> <span class="title class_">Path</span>(dst), <span class="keyword">new</span> <span class="title class_">Progressable</span>() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">progress</span><span class="params">()</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;上传完一个设定缓存区大小容量的文件！&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        IOUtils.copyBytes(in, out, cf);</span><br><span class="line">        System.out.println(<span class="string">&quot;LocalFile:\t&quot;</span> + localstr+<span class="string">&quot;\tupload to:&quot;</span> + dst);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">downloadFile</span><span class="params">(Configuration cf, String remoteStr, String localString)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span>FileSystem.get(hdfsHost,cf); </span><br><span class="line">        <span class="type">InputStream</span> <span class="variable">in</span> <span class="operator">=</span> fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(remoteStr));</span><br><span class="line">        <span class="type">OutputStream</span> <span class="variable">out</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(localString);</span><br><span class="line">        IOUtils.copyBytes(in, out, cf);</span><br><span class="line">        System.out.println(<span class="string">&quot;downloadFile:\t&quot;</span>+remoteStr +<span class="string">&quot;to &quot;</span> + localString);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">cf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">String</span> <span class="variable">path</span> <span class="operator">=</span> <span class="string">&quot;/tmp/dem0.txt&quot;</span>;</span><br><span class="line">        cf.set(<span class="string">&quot;ds.defaultFs&quot;</span>,<span class="string">&quot;hdfs://&quot;</span>+nameNode);</span><br><span class="line">        System.out.println(<span class="string">&quot;[*]createHelloWorld:&quot;</span>);</span><br><span class="line">        createHelloWorld(cf,path);</span><br><span class="line">        System.out.println(<span class="string">&quot;[*]showDir:&quot;</span>);</span><br><span class="line">        showDir(cf, <span class="string">&quot;/tmp&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;[*]fileExist:&quot;</span>);</span><br><span class="line">        fileExist(cf, path);</span><br><span class="line">        System.out.println(<span class="string">&quot;[*]readFile:&quot;</span>);</span><br><span class="line">        readFile(cf, path);</span><br><span class="line">        System.out.println(<span class="string">&quot;[*]delteFile:&quot;</span>);</span><br><span class="line">        delteFile(cf, path);</span><br><span class="line">        System.out.println(<span class="string">&quot;[*]showDir:&quot;</span>);</span><br><span class="line">        showDir(cf, <span class="string">&quot;/&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;[*]uploadFile:&quot;</span>);</span><br><span class="line">        uploadFile(cf, <span class="string">&quot;/etc/passwd&quot;</span>, <span class="string">&quot;/tmp/passwd&quot;</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-2-vcode-maven-hadoop开发环境配置"><a href="#2-2-vcode-maven-hadoop开发环境配置" class="headerlink" title="2.2 vcode+maven+hadoop开发环境配置"></a>2.2 vcode+maven+hadoop开发环境配置</h3><p>基于<code>iotdevelop</code>环境</p>
<ul>
<li><p>安装vcode</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://vscode.cdn.azure.cn/stable/4af164ea3a06f701fe3e89a2bcbb421d2026b68f/code_1.68.0-1654690107_amd64.deb?1 -o code.deb</span><br><span class="line">sudo dpkg -i ./code.deb</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装maven</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install maven</span><br><span class="line"><span class="built_in">export</span> M2_HOME=/usr/share/maven(这一句加入/etc/profile)</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置maven的阿里源</p>
<p>详情参照参考资料</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo vim /usr/share/maven/conf/settings.xml </span><br></pre></td></tr></table></figure>

<p><img src="https://img.dem0dem0.top/images/image-20220614151033999.png" alt="image-20220614151033999"></p>
</li>
<li><p>配置vcode</p>
<p>下载插件<code>Java Extension Pack</code>.</p>
<p><img src="https://img.dem0dem0.top/images/image-20220614161346645.png" alt="image-20220614161346645"></p>
<p>开始配置</p>
<p><img src="https://img.dem0dem0.top/images/image-20220614161426059.png" alt="image-20220614161426059"></p>
<p><img src="https://img.dem0dem0.top/images/image-20220614161441367.png" alt="image-20220614161441367"></p>
</li>
<li><p>新建项目</p>
<p>在空白区域<code>右键</code></p>
<p><img src="https://img.dem0dem0.top/images/image-20220614161523396.png" alt="image-20220614161523396"></p>
<p><img src="https://img.dem0dem0.top/images/image-20220614161540381.png" alt="image-20220614161540381"></p>
<p><img src="https://img.dem0dem0.top/images/image-20220614161556634.png" alt="image-20220614161556634"></p>
</li>
</ul>
<p>​	剩下两个选项输入自己想要输入的内容。点击</p>
<p><img src="https://img.dem0dem0.top/images/image-20220614161628479.png" alt="image-20220614161628479"></p>
<p>选择你要存放的目录。然后需要等待。</p>
<p><img src="https://img.dem0dem0.top/images/image-20220614161807103.png" alt="image-20220614161807103"></p>
<p>直接回车就可以了。然后导入依赖。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 导入hadoop依赖环境 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-yarn-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><img src="https://img.dem0dem0.top/images/image-20220614162024223.png" alt="image-20220614162024223"></p>
<p>注意所处的位置!!然后等待就行了。加载完毕之后就可以写代码了。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote>
<p>1.hdfs命令行操作:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/271098213">https://zhuanlan.zhihu.com/p/271098213</a></p>
<p>2.hdfs代码操作：<a target="_blank" rel="noopener" href="https://blog.csdn.net/little_sloth/article/details/107040607">https://blog.csdn.net/little_sloth/article/details/107040607</a></p>
<p>3.vcode+maven+hadoop开发:<a target="_blank" rel="noopener" href="https://www.cnblogs.com/orion-orion/p/15664772.html">https://www.cnblogs.com/orion-orion/p/15664772.html</a></p>
<p>4.ubuntu安装maven:<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1649751">https://cloud.tencent.com/developer/article/1649751</a></p>
<p>5.java权限:<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43541746/article/details/115422142">https://blog.csdn.net/qq_43541746/article/details/115422142</a></p>
<p>6.mapreduce入门:<a target="_blank" rel="noopener" href="https://www.runoob.com/w3cnote/mapreduce-coding.html">https://www.runoob.com/w3cnote/mapreduce-coding.html</a></p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://dem0dem0.top">Dem0</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://dem0dem0.top/2022/06/13/hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">https://dem0dem0.top/2022/06/13/hadoop环境配置/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://dem0dem0.top" target="_blank">Dem0のblog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/">大数据处理</a></div><div class="post_share"><div class="social-share" data-image="https://img.dem0dem0.top/images/image-20220618082641264.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/06/18/MapReduce%E4%B8%89%E4%B8%AA%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/"><img class="prev-cover" src="https://img.dem0dem0.top/images/image-20220618093502409.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">MapReduce三个入门案例</div></div></a></div><div class="next-post pull-right"><a href="/2022/06/12/Thymeleaf%E6%8E%A2%E9%99%A9/"><img class="next-cover" src="/img/cover1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Thymeleaf探险</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/06/18/MapReduce%E4%B8%89%E4%B8%AA%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/" title="MapReduce三个入门案例"><img class="cover" src="https://img.dem0dem0.top/images/image-20220618093502409.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-18</div><div class="title">MapReduce三个入门案例</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://q1.qlogo.cn/g?b=qq&amp;nk=79475432&amp;s=640" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Dem0</div><div class="author-info__description">一个什么都想学的web🐕</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/3em0"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="/atom.xml" target="_blank" title="RSS链接"><i class="fa fa-rss"></i></a><a class="social-icon" href="https://github.com/3em0" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=79475432&amp;website=www.oicqzone.com" target="_blank" title="Github"><i class="fab fa-qq"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Misfortune meets love; Maintain love for network security;</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop"><span class="toc-number">1.</span> <span class="toc-text">Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E8%BF%87%E7%A8%8B"><span class="toc-number">1.1.</span> <span class="toc-text">1 安装配置过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E6%A0%91%E8%8E%93%E6%B4%BE"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 安装配置树莓派</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEjdk"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 安装配置jdk</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEhadoop"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 安装配置hadoop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E9%AA%8C%E8%AF%81%E5%AE%89%E8%A3%85"><span class="toc-number">1.1.4.</span> <span class="toc-text">1.4 验证安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-%E4%BA%A4%E6%8D%A2%E6%9C%BA%E9%85%8D%E7%BD%AE-%E5%8F%AF%E9%80%89"><span class="toc-number">1.1.5.</span> <span class="toc-text">1.5 交换机配置(可选)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3"><span class="toc-number">1.1.6.</span> <span class="toc-text">1.6 问题解决</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#a-JAVA-HOME%E6%B2%A1%E6%9C%89%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.1.6.1.</span> <span class="toc-text">a. JAVA_HOME没有设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#b-master-Permission-denied"><span class="toc-number">1.1.6.2.</span> <span class="toc-text">b. master: Permission denied</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#c-%E8%A7%A3%E5%86%B3could-only-be-written-to-0-of-the-1-minReplicati"><span class="toc-number">1.1.6.3.</span> <span class="toc-text">c. 解决could only be written to 0 of the 1 minReplicati</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-hadoop%E4%BD%BF%E7%94%A8"><span class="toc-number">1.2.</span> <span class="toc-text">2. hadoop使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-hadoop%E6%96%87%E4%BB%B6%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 hadoop文件基础操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SHELL-%E5%91%BD%E4%BB%A4"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">SHELL 命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#java%E4%BB%A3%E7%A0%81%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">java代码操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-vcode-maven-hadoop%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 vcode+maven+hadoop开发环境配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.3.</span> <span class="toc-text">参考资料</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/07/08/XXE%E5%A4%87%E5%BF%98%E5%BD%95/" title="XXE备忘录"><img src="/img/cover5.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="XXE备忘录"/></a><div class="content"><a class="title" href="/2022/07/08/XXE%E5%A4%87%E5%BF%98%E5%BD%95/" title="XXE备忘录">XXE备忘录</a><time datetime="2022-07-08T02:54:17.000Z" title="发表于 2022-07-08 10:54:17">2022-07-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/22/%E6%9F%90ciscn%E5%88%86%E5%8C%BA%E4%B8%A4%E4%B8%AA%E9%A2%98%E8%A7%A3/" title="某ciscn分区两个题解"><img src="/img/conver3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="某ciscn分区两个题解"/></a><div class="content"><a class="title" href="/2022/06/22/%E6%9F%90ciscn%E5%88%86%E5%8C%BA%E4%B8%A4%E4%B8%AA%E9%A2%98%E8%A7%A3/" title="某ciscn分区两个题解">某ciscn分区两个题解</a><time datetime="2022-06-21T23:53:58.000Z" title="发表于 2022-06-22 07:53:58">2022-06-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/18/MapReduce%E4%B8%89%E4%B8%AA%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/" title="MapReduce三个入门案例"><img src="https://img.dem0dem0.top/images/image-20220618093502409.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MapReduce三个入门案例"/></a><div class="content"><a class="title" href="/2022/06/18/MapReduce%E4%B8%89%E4%B8%AA%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/" title="MapReduce三个入门案例">MapReduce三个入门案例</a><time datetime="2022-06-17T16:29:51.000Z" title="发表于 2022-06-18 00:29:51">2022-06-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/13/hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" title="Hadoop环境配置"><img src="https://img.dem0dem0.top/images/image-20220618082641264.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hadoop环境配置"/></a><div class="content"><a class="title" href="/2022/06/13/hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" title="Hadoop环境配置">Hadoop环境配置</a><time datetime="2022-06-13T12:10:23.000Z" title="发表于 2022-06-13 20:10:23">2022-06-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/12/Thymeleaf%E6%8E%A2%E9%99%A9/" title="Thymeleaf探险"><img src="/img/cover1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Thymeleaf探险"/></a><div class="content"><a class="title" href="/2022/06/12/Thymeleaf%E6%8E%A2%E9%99%A9/" title="Thymeleaf探险">Thymeleaf探险</a><time datetime="2022-06-12T03:32:43.000Z" title="发表于 2022-06-12 11:32:43">2022-06-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Dem0</div><div class="footer_custom_text">努力不一定能实现梦想，但是曾经努力过的事实却足以安慰自己</br><p><a style="margin-inline:5px"target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为 Hexo" alt="HEXO"></a><a style="margin-inline:5px"target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用 Butterfly" alt="Butterfly"></a><a style="margin-inline:5px"target="_blank" href="https://cloud.tencent.com/"><img src="https://img.shields.io/badge/cdn-Tencent-orange" title="本站使用 Tencent 为静态资源提供CDN加速" alt="Tencent"></a><a style="margin-inline:5px"target="_blank" href="https://github.com/3em0"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由 GitHub 托管" alt="GitHub"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" alt="img" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: 'c752b82ba7eb272e07ff',
      clientSecret: '2032517bf35f44c10890d7bd2d1daf1dc35ce43e',
      repo: '3em0.github.io',
      owner: '3em0',
      admin: ['3em0'],
      id: 'f0a5f69f34c160952cca5e3f72e2c694',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>